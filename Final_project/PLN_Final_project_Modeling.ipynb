{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9345482a-acb3-4d56-9d72-63c63e3cbc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOPIC MODELING\n",
    "#LDA topic modeling with sklearn\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "268daea9-2c53-4c10-9c7d-e98fed5c3e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4e4c9dc8-4859-416c-810d-09bce5a1cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1660b953-6db9-43a5-b15a-ce352ef49d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/dariamartinovskaya/Downloads/PLN')\n",
    "from Chapter04.preprocess_bbc_dataset import get_stopwords\n",
    "stopwords_file_path = \"/Users/dariamartinovskaya/Downloads/PLN/Chapter01/stopwords.csv\"\n",
    "stopwords = get_stopwords(stopwords_file_path)\n",
    "bbc_dataset = \"/Users/dariamartinovskaya/Downloads/PLN/Chapter04/bbc-text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f4b579ab-31bc-4068-9f95-b9ca596e5311",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/dariamartinovskaya/Downloads/PLN')\n",
    "from Chapter04.unsupervised_text_classification import tokenize_and_stem\n",
    "stopwords_file_path = \"/Users/dariamartinovskaya/Downloads/PLN/Chapter01/stopwords.csv\"\n",
    "stopwords = get_stopwords(stopwords_file_path)\n",
    "bbc_dataset = \"/Users/dariamartinovskaya/Downloads/PLN/Chapter04/bbc-text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ab873eb-3df7-4795-84b0-e1786fc4aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_vectorizer(documents):\n",
    "    count_vectorizer = \\\n",
    "    CountVectorizer(stop_words=stopwords, tokenizer=tokenize_and_stem)\n",
    "    data = count_vectorizer.fit_transform(documents)\n",
    "    return (count_vectorizer, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a659ad2d-13bd-4527-ae5a-ebf1cb9121bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df['text'] = \\\n",
    "    df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
    "    df['text'] = \\\n",
    "    df['text'].apply(lambda x: re.sub(r'\\d', '', x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d7dc34d2-8b51-4dd3-bce2-9fed3e166c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_fit_lda(data, num_topics):\n",
    "    lda = LDA(n_components=num_topics, n_jobs=-1)\n",
    "    lda.fit(data)\n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "017b1f97-8888-4c12-9bdd-4b30c57e2d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_words_for_topics(model, vectorizer, n_top_words):\n",
    "    words = vectorizer.get_feature_names_out() \n",
    "    word_dict = {}\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        word_dict[f\"Topic {topic_index}\"] = [words[i] for i in topic.argsort()[-n_top_words:]]\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f64849ba-47fe-481d-b643-10f44a3d3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topic_words(word_dict):\n",
    "    for key in word_dict.keys():\n",
    "        print(f\"Topic {key}\")\n",
    "        print(\"\\t\", word_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "945af3e7-ecb4-4ede-9eb4-5fddaf1de3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(bbc_dataset)\n",
    "df = clean_data(df)\n",
    "documents = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2d19efd5-22a2-4c86-8434-47c272627338",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_topics = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1cd2eb67-bfe2-4b93-8a6d-c71e9037f913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dariaenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/dariaenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anywh', 'el', 'elsewh', 'everywh', 'l', 'otherwi', 'plea', 'somewh', 'v', 'wor'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "(vectorizer, data) = create_count_vectorizer(documents)\n",
    "lda = create_and_fit_lda(data, number_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e51f12f4-50fa-4762-981d-4844a4274324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Topic 0\n",
      "\t ['servic', 'phone', 'make', 'mobil', 'new', 'technolog', 'music', 'game', 'peopl', 'use']\n",
      "Topic Topic 1\n",
      "\t ['tori', 'blair', 'plan', 'peopl', 'minist', 'parti', 'labour', 'elect', 'say', 'govern']\n",
      "Topic Topic 2\n",
      "\t ['player', 'wale', 'back', 'world', 'first', 'win', 'game', 'play', 'england', 'year']\n",
      "Topic Topic 3\n",
      "\t ['month', 'share', 'price', 'sale', 'm', 'firm', 'market', 'compani', 'bn', 'year']\n",
      "Topic Topic 4\n",
      "\t ['won', 'play', 'win', 'time', 'star', 'award', 'best', 'year', 'm', 'film']\n"
     ]
    }
   ],
   "source": [
    "topic_words = \\\n",
    "get_most_common_words_for_topics(lda, vectorizer, 10)\n",
    "print_topic_words(topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "efab783b-f4ec-41f2-bed1-09f11e376a71",
   "metadata": {},
   "outputs": [],
   "source": [
    " import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "61b94469-8d05-43d9-8bf3-4103f66f52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Users/dariamartinovskaya/Downloads/PLN/Chapter06/lda_sklearn.pkl\"\n",
    "vectorizer_path = \"/Users/dariamartinovskaya/Downloads/PLN/Chapter06/vectorizer.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "313e8077-9b32-4e85-ad56-3cdb3811da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_example = \"\"\"Manchester United players slumped to the turf at full-time in Germany on Tuesday in acknowledgement of what their latest pedestrian first- half display had cost them. The 3-2 loss at RB Leipzig means United will not be one of the 16 teams in the draw for the knockout stages of the Champions League. And this is not the only price for failure. The damage will be felt in the accounts, in the dealings they have with current and potentially future players and in the faith the fans have placed in manager Ole Gunnar Solskjaer. With Paul Pogba's agent angling for a move for his client and ex-United defender Phil Neville speaking of a \"witchhunt\" against his former team-mate Solskjaer, BBC Sport looks at the ramifications and reaction to a big loss for United.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "35ac34f7-929d-4843-8ba1-34f3c165c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(lda, lda_path, vect, vect_path):\n",
    "    pickle.dump(lda, open(lda_path, 'wb'))\n",
    "    pickle.dump(vect, open(vect_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d3d60a9e-5438-47f8-96a0-10142c92fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new_example(lda, vect, example):\n",
    "    vectorized = vect.transform([example])\n",
    "    topic = lda.transform(vectorized)\n",
    "    print(topic)\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "af8fa2a2-01dd-493c-8d51-dc76421c6a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00345405 0.06308028 0.03867231 0.23318821 0.66160516]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00345405, 0.06308028, 0.03867231, 0.23318821, 0.66160516]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new_example(lda, vectorizer, new_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c32ce9c2-fcce-4405-91dc-fcefe0a6c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA topic modeling with gensim\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# from Chapter06.lda_topic import stopwords, bbc_dataset, clean_data\n",
    "\n",
    "curerntPath = os.getcwd() \n",
    "stopwords_file_path = \"/Users/dariamartinovskaya/Downloads/PLN/Chapter01/stopwords.csv\"\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b63be-0f5f-41aa-bc10-318e7510e07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b97c1-d31f-4dc8-8d9e-df6d5720a00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "41a75af3-292f-4b46-a235-ca8ad0c55e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_csv(filename):\n",
    "    file = open(filename, \"r\", encoding=\"utf-8\")\n",
    "    text = file.read()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    words = nltk.tokenize.word_tokenize(text)\n",
    "    freq_dist = FreqDist(word.lower() for word in words)\n",
    "    words_with_frequencies = [(word, freq_dist[word]) for word in freq_dist.keys()]\n",
    "    sorted_words = sorted(words_with_frequencies, key=lambda tup: tup[1])\n",
    "    length_cutoff = int(0.02*len(sorted_words))\n",
    "    stopwords = [tuple[0] for tuple in sorted_words[-length_cutoff:]]\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b67571bd-f1be-4b71-967a-50b2745a1e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords(path=stopwords_file_path):\n",
    "    stopwords = read_in_csv(path)\n",
    "    stopwords = [word[0] for word in stopwords]\n",
    "    stemmed_stopwords = [stemmer.stem(word) for word in stopwords]\n",
    "    stopwords = stopwords + stemmed_stopwords\n",
    "    return stopwords\n",
    "\n",
    "stopwords = get_stopwords(stopwords_file_path)\n",
    "bbc_dataset = os.path.join(curerntPath, \"/Users/dariamartinovskaya/Downloads/PLN/Chapter04/bbc-text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0be35fc8-f912-4cee-9b74-ba6967019fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'\\d', '', x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "00b1a273-3997-476e-b886-46863012b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = clean_data(df)\n",
    "    df['text'] = df['text'].apply(lambda x: simple_preprocess(x, deacc=True))\n",
    "    df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stopwords])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a7a6615d-a97c-4c46-8a7e-86d0d2f4ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lda_model(id_dict, corpus, num_topics):\n",
    "    lda_model = LdaModel(corpus=corpus,\n",
    "        id2word=id_dict,\n",
    "        num_topics=num_topics,\n",
    "        random_state=100,\n",
    "        chunksize=100,\n",
    "        passes=10)\n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "879ba836-89ef-49a7-8b04-c05fb25bc3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(bbc_dataset)\n",
    "df = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0f4a8703-a70c-4c68-af59-d594651b81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text'].values\n",
    "id_dict = corpora.Dictionary(texts)\n",
    "corpus = [id_dict.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5f3dc702-e659-4e25-9df5-de4bd47d16f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_topics = 5\n",
    "lda_model = create_lda_model(id_dict, corpus, number_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c6e035a2-82a5-4319-9468-9556a7b2fd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.059*\"the\" + 0.020*\"and\" + 0.020*\"in\" + 0.019*\"of\" + 0.018*\"film\" + '\n",
      "  '0.015*\"for\" + 0.012*\"best\" + 0.009*\"was\" + 0.009*\"at\" + 0.007*\"awards\"'),\n",
      " (1,\n",
      "  '0.070*\"the\" + 0.036*\"to\" + 0.028*\"of\" + 0.022*\"and\" + 0.017*\"in\" + '\n",
      "  '0.013*\"said\" + 0.012*\"that\" + 0.010*\"for\" + 0.010*\"is\" + 0.010*\"on\"'),\n",
      " (2,\n",
      "  '0.056*\"the\" + 0.033*\"to\" + 0.029*\"of\" + 0.025*\"and\" + 0.017*\"in\" + '\n",
      "  '0.014*\"that\" + 0.013*\"is\" + 0.012*\"it\" + 0.010*\"are\" + 0.010*\"for\"'),\n",
      " (3,\n",
      "  '0.059*\"the\" + 0.030*\"to\" + 0.024*\"in\" + 0.023*\"and\" + 0.017*\"of\" + '\n",
      "  '0.013*\"he\" + 0.011*\"it\" + 0.011*\"for\" + 0.010*\"on\" + 0.010*\"is\"'),\n",
      " (4,\n",
      "  '0.061*\"the\" + 0.038*\"in\" + 0.025*\"of\" + 0.022*\"to\" + 0.015*\"and\" + '\n",
      "  '0.011*\"its\" + 0.011*\"bn\" + 0.010*\"said\" + 0.010*\"at\" + 0.009*\"us\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3b839b67-bfe8-4f0b-8b7f-5adaaeb2c7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_example = \"\"\"Manchester United players slumped to the turf at full-time in Germany on Tuesday in acknowledgement of what their latest pedestrian first-half display had cost them. The 3-2 loss at RB Leipzig means United will not be one of the 16 teams in the draw for the knockout stages of the Champions League. And this is not the only price for failure. The damage will be felt in the accounts, in the dealings they have with current and potentially future players and in the faith the fans have placed in manager Ole Gunnar Solskjaer. With Paul Pogba's agent angling for a move for his client and ex-United defender Phil Neville speaking of a \"witchhunt\" against his former team-mate Solskjaer, BBC Sport looks at the ramifications and reaction to a big loss for United.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2b1a7859-2f8d-46ac-b0fc-a09e6ff7e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(lda, lda_path, id_dict, dict_path):\n",
    "    lda.save(lda_path)\n",
    "    id_dict.save(dict_path)\n",
    "\n",
    "corpus = [id_dict.doc2bow(text) for text in texts]\n",
    "id_dict = corpora.Dictionary(texts)\n",
    "lda_model = create_lda_model(id_dict, corpus, number_topics)\n",
    "lda_path = \"/Users/dariamartinovskaya/Downloads/lda_model\"\n",
    "dict_path = \"/Users/dariamartinovskaya/Downloads/dictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d6071204-b611-4605-8b2e-e1c4324282e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(lda_path, dict_path):\n",
    "    lda = LdaModel.load(lda_path)\n",
    "    id_dict = corpora.Dictionary.load(dict_path)\n",
    "    return (lda, id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7aed6a41-8067-4539-9397-5a6a2d075363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new_example(lda, id_dict, input_string):\n",
    "    input_list = clean_text(input_string)\n",
    "    bow = id_dict.doc2bow(input_list)\n",
    "    topics = lda[bow]\n",
    "    print(topics)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c0ecaade-7276-45fd-a58d-a62f7aca1156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dariamartinovskaya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dariamartinovskaya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(input_string):\n",
    "    input_string = input_string.lower()\n",
    "    \n",
    "    input_string = re.sub(r'[^\\w\\s]', '', input_string)\n",
    "    \n",
    "    tokens = word_tokenize(input_string)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "10486fc5-c59f-4fe1-990f-bc71fc80a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(lda_model, model_path, id_dict, dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f956f638-3dd6-4308-97ec-1aa4462c5bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.17702451), (3, 0.7484408), (4, 0.06738019)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 0.17702451), (3, 0.7484408), (4, 0.06738019)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new_example(lda_model, id_dict, new_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
